{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (1.11.0)\n",
      "Requirement already up-to-date: torchdata==0.3.0 in /root/.local/lib/python3.7/site-packages (0.3.0)\n",
      "Collecting torchtext==0.12.0\n",
      "  Downloading torchtext-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0) (3.7.4.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (4.43.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12.0) (1.21.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (3.0.4)\n",
      "Installing collected packages: torchtext\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.14.1\n",
      "    Uninstalling torchtext-0.14.1:\n",
      "      Successfully uninstalled torchtext-0.14.1\n",
      "Successfully installed torchtext-0.12.0\n",
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch==1.11.0 torchdata==0.3.0 torchtext==0.12.0\n",
    "import torchtext\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding tokens:  15173\n",
      "                                              title  \\\n",
      "0                                           Beyoncé   \n",
      "1                                   Frédéric_Chopin   \n",
      "2    Sino-Tibetan_relations_during_the_Ming_dynasty   \n",
      "3                                              IPod   \n",
      "4            The_Legend_of_Zelda:_Twilight_Princess   \n",
      "..                                              ...   \n",
      "437                                       Infection   \n",
      "438                                         Hunting   \n",
      "439                                       Kathmandu   \n",
      "440                           Myocardial_infarction   \n",
      "441                                          Matter   \n",
      "\n",
      "                                            paragraphs  \n",
      "0    [{'qas': [{'question': 'When did Beyonce start...  \n",
      "1    [{'qas': [{'question': \"What was Frédéric's na...  \n",
      "2    [{'qas': [{'question': 'Who were Wang Jiawei a...  \n",
      "3    [{'qas': [{'question': 'Which company produces...  \n",
      "4    [{'qas': [{'question': 'What category of game ...  \n",
      "..                                                 ...  \n",
      "437  [{'qas': [{'question': 'Of the huge amount of ...  \n",
      "438  [{'qas': [{'question': 'What is the practice o...  \n",
      "439  [{'qas': [{'question': 'What country is Kathma...  \n",
      "440  [{'qas': [{'plausible_answers': [{'text': 'Myo...  \n",
      "441  [{'qas': [{'plausible_answers': [{'text': 'ord...  \n",
      "\n",
      "[442 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# below added by me\n",
    "from torchtext.datasets import SQuAD2\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# below added by me\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torch import nn\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save('brown.embedding')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "print(\"Before adding tokens: \", len(w2v.wv))\n",
    "# print(w2v.wv)\n",
    "# print(\"vector len of a: \", len(w2v.wv.get_vector('an')))\n",
    "w2v.wv.add_vectors(['SOS', 'EOS'], [np.zeros(100), np.zeros(100)])\n",
    "# print(\"After token added: \", len(w2v.wv))\n",
    "# print(w2v.wv)\n",
    "weights = torch.FloatTensor(w2v.wv.vectors)\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        data = json.loads(l)    \n",
    "        for datum in data['data']:\n",
    "            yield datum\n",
    "\n",
    "def loadDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    nltkTokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = nltkTokenizer.tokenize(sentence)\n",
    "\n",
    "    tokens.append('EOS')\n",
    "    tokens.insert(0, 'SOS')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split(dataset):\n",
    "    \n",
    "    '''\n",
    "    Input: SRC, our list of questions from the dataset \n",
    "            TRG, our list of responses from the dataset\n",
    "\n",
    "    Output: Training and test datasets for SRC & TRG\n",
    "\n",
    "    '''\n",
    "    train_size = int(len(dataset)* 0.8)\n",
    "    validation_size = int(len(dataset)* 0.1)\n",
    "    test_size = len(dataset) - train_size - validation_size\n",
    "\n",
    "    train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "SRC_train_iter = SQuAD2(root = './', split='train')\n",
    "df = loadDF('./SQuAD2/train-v2.0.json.gz')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.index = {}\n",
    "        self.count = 0\n",
    "        self.words = {}\n",
    "        self.words_freq = {}\n",
    "\n",
    "    def indexWord(self, word):\n",
    "        if word not in self.words:\n",
    "            self.words[word] = self.count\n",
    "            self.words_freq[word] = 1\n",
    "            self.index[str(self.count)] = word\n",
    "            self.count += 1\n",
    "            return True\n",
    "        else:\n",
    "            self.words_freq[word] += 1\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding word 0 to our vocabulary.\n",
      "['SOS', 'When', 'did', 'Beyonce', 'start', 'becoming', 'popular', 'EOS']\n",
      "Adding word 1000 to our vocabulary.\n",
      "['SOS', 'Besides', 'R', 'B', 'which', 'genres', 'does', 'Beyonce', 'dabble', 'in', 'EOS']\n",
      "Adding word 2000 to our vocabulary.\n",
      "['SOS', 'Which', 'friend', 'took', 'on', 'the', 'role', 'of', 'several', 'jobs', 'to', 'help', 'Chopin', 'including', 'copyist', 'EOS']\n",
      "Adding word 3000 to our vocabulary.\n",
      "['SOS', 'What', 'was', 'the', 'name', 'of', 'the', 'Tibetologist', 'EOS']\n",
      "Adding word 4000 to our vocabulary.\n",
      "['SOS', 'Many', 'different', 'types', 'of', 'interaction', 'can', 'be', 'controlled', 'by', 'how', 'many', 'buttons', 'EOS']\n",
      "Adding word 5000 to our vocabulary.\n",
      "['SOS', 'delayed', 'arrival', 'EOS']\n",
      "Adding word 6000 to our vocabulary.\n",
      "['SOS', 'About', 'how', 'many', 'million', 'square', 'feet', 'of', 'office', 'space', 'is', 'present', 'in', 'Midtown', 'Manhattan', 'EOS']\n",
      "Adding word 7000 to our vocabulary.\n",
      "['SOS', 'hydrogen', 'production', 'from', 'protons', 'EOS']\n",
      "Adding word 8000 to our vocabulary.\n",
      "['SOS', 'What', 'mentions', 'taking', 'refuge', 'on', 'behalf', 'of', 'young', 'or', 'unborn', 'children', 'EOS']\n",
      "Adding word 9000 to our vocabulary.\n",
      "['SOS', 'Natural', 'selection', 'and', 'what', 'makes', 'certain', 'dogs', 'behave', 'certain', 'ways', 'EOS']\n",
      "Adding word 10000 to our vocabulary.\n",
      "['SOS', 'Who', 'were', 'responsible', 'for', 'the', 'revolts', 'that', 'ended', 'with', 'Youlou', 's', 'removal', 'EOS']\n",
      "Adding word 11000 to our vocabulary.\n",
      "['SOS', 'What', 'is', 'the', 'word', 'for', 'the', 'kind', 'of', 'relationship', 'in', 'which', 'a', 'plant', 'depend', 'on', 'a', 'single', 'type', 'of', 'insect', 'EOS']\n",
      "Adding word 12000 to our vocabulary.\n",
      "['SOS', 'At', 'what', 'century', 's', 'start', 'did', 'revivalist', 'fall', 'into', 'disfavor', 'EOS']\n",
      "Adding word 13000 to our vocabulary.\n",
      "['SOS', 'Congregation', 'for', 'the', 'Doctrine', 'of', 'the', 'Faith', 'EOS']\n",
      "Adding word 14000 to our vocabulary.\n",
      "['SOS', 'What', 'year', 'was', 'the', 'RSFSW', 'nor', 'recognized', 'as', 'an', 'independent', 'state', 'EOS']\n",
      "Adding word 15000 to our vocabulary.\n",
      "['SOS', 'loss', 'of', 'full', 'passports', 'EOS']\n",
      "Adding word 16000 to our vocabulary.\n",
      "['SOS', 'chivalry', 'EOS']\n",
      "Adding word 17000 to our vocabulary.\n",
      "['SOS', 'What', 'is', 'CCM', 'an', 'acronym', 'of', 'EOS']\n",
      "Adding word 18000 to our vocabulary.\n",
      "['SOS', '59', '948', 'EOS']\n",
      "Adding word 19000 to our vocabulary.\n",
      "['SOS', 'What', 'county', 'did', 'the', 'city', 'of', 'Southampton', 'become', 'administratively', 'independent', 'of', 'in', 'April', 'of', '1997', 'EOS']\n",
      "Adding word 20000 to our vocabulary.\n",
      "['SOS', 'What', 'were', 'Octavian', 's', 'agent', 'd', 'affaires', 'called', 'EOS']\n",
      "Adding word 21000 to our vocabulary.\n",
      "['SOS', 'MEG', 'of', 'the', 'brain', 'is', 'an', 'abbreviation', 'of', 'what', 'EOS']\n",
      "Adding word 22000 to our vocabulary.\n",
      "['SOS', 'Mohammed', 'Awale', 'Liban', 'EOS']\n",
      "Adding word 23000 to our vocabulary.\n",
      "['SOS', 'New', 'Delhi', 'is', 'well', 'known', 'for', 'what', 'type', 'of', 'beautifully', 'landscaped', 'feature', 'EOS']\n",
      "Adding word 24000 to our vocabulary.\n",
      "['SOS', 'Plaza', 'de', 'Armas', 'EOS']\n",
      "Adding word 25000 to our vocabulary.\n",
      "['SOS', 'Ctenophora', 'and', 'the', 'Cnidaria', 'include', 'what', 'sea', 'creatures', 'EOS']\n",
      "Adding word 26000 to our vocabulary.\n",
      "['SOS', 'What', 'retired', 'NASA', 'Office', 'of', 'inspector', 'general', 'is', 'outspoken', 'about', 'the', 'FAA', 'EOS']\n",
      "Adding word 27000 to our vocabulary.\n",
      "['SOS', 'forming', 'spherical', 'structures', 'made', 'of', 'a', 'lipid', 'bilayer', 'enclosing', 'periplasmic', 'materials', 'EOS']\n",
      "Adding word 28000 to our vocabulary.\n",
      "['SOS', 'the', 'PowerBook', 'line', 'EOS']\n",
      "Adding word 29000 to our vocabulary.\n",
      "['SOS', 'Which', 'idea', 'is', 'known', 'as', 'the', 'ability', 'of', 'a', 'person', 'to', 'have', 'opinions', 'and', 'beliefs', 'that', 'are', 'defined', 'confidently', 'consistent', 'and', 'stable', 'EOS']\n",
      "Adding word 30000 to our vocabulary.\n",
      "['SOS', 'maternal', 'immunization', 'hypothesis', 'EOS']\n",
      "Adding word 31000 to our vocabulary.\n",
      "['SOS', '1726', 'EOS']\n",
      "Adding word 32000 to our vocabulary.\n",
      "['SOS', '1731', 'EOS']\n",
      "Adding word 33000 to our vocabulary.\n",
      "['SOS', 'What', 'creature', 'is', 'swimming', 'in', 'the', 'Dutch', 'phrase', 'Visc', 'flot', 'aftar', 'themo', 'uuatare', 'EOS']\n",
      "Adding word 34000 to our vocabulary.\n",
      "['SOS', 'What', 'percent', 'of', 'its', 'stake', 'in', 'Woodside', 'Petroleum', 'did', 'Shell', 'sell', 'in', 'June', '2014', 'EOS']\n",
      "Adding word 35000 to our vocabulary.\n",
      "['SOS', 'Who', 'was', 'given', 'the', 'prilavage', 'of', 'being', 'called', 'the', 'first', 'King', 'of', 'the', 'Germans', 'EOS']\n",
      "Adding word 36000 to our vocabulary.\n",
      "['SOS', 'De', 'Materia', 'Medica', 'EOS']\n",
      "Adding word 37000 to our vocabulary.\n",
      "['SOS', 'Many', 'towns', 'have', 'encouraged', 'small', 'scale', 'light', 'industries', 'such', 'as', 'Crewkerne', 's', 'Ariel', 'Motor', 'Company', 'one', 'of', 'the', 'UK', 's', 'smallest', 'car', 'manufacturers', 'EOS']\n",
      "Adding word 38000 to our vocabulary.\n",
      "['SOS', 'What', 'book', 'of', 'poetry', 'was', 'published', 'by', 'John', 'Kneubuhl', 'in', '2004', 'EOS']\n",
      "Adding word 39000 to our vocabulary.\n",
      "['SOS', 'What', 'is', 'guwen', 'considered', 'as', 'EOS']\n",
      "Adding word 40000 to our vocabulary.\n",
      "['SOS', 'Ricci', 'v', 'DeStefano', 'EOS']\n",
      "Adding word 41000 to our vocabulary.\n",
      "['SOS', 'MRP', 'AEG', 'group', 'EOS']\n",
      "Adding word 42000 to our vocabulary.\n",
      "['SOS', 'Cork', 'Opera', 'House', 'capacity', 'c', '1000', 'Cyprus', 'Avenue', 'Triskel', 'Christchurch', 'the', 'Roundy', 'the', 'Savoy', 'and', 'Coughlan', 's', 'EOS']\n",
      "Adding word 43000 to our vocabulary.\n",
      "['SOS', 'Helms', 'Athletic', 'Foundation', 'EOS']\n",
      "Adding word 44000 to our vocabulary.\n",
      "['SOS', 'How', 'many', 'Synergistic', 'Processing', 'Elements', 'are', 'in', 'a', 'PS3', 's', 'CPU', 'EOS']\n",
      "Adding word 45000 to our vocabulary.\n",
      "['SOS', 'Tongass', 'National', 'Forest', 'EOS']\n",
      "Adding word 46000 to our vocabulary.\n",
      "['SOS', 'which', 'spring', 'emitted', 'vapors', 'that', 'caused', 'the', 'oracle', 'at', 'Delphi', 'to', 'give', 'her', 'prophecies', 'EOS']\n",
      "Adding word 47000 to our vocabulary.\n",
      "['SOS', 'Glagolitic', 'Missale', 'Romanum', 'Glagolitice', '1483', 'EOS']\n",
      "Adding word 48000 to our vocabulary.\n",
      "['SOS', 'Jodhpur', 'Jaisalmer', 'Barmer', 'Bikaner', 'and', 'Nagour', 'EOS']\n",
      "Adding word 49000 to our vocabulary.\n",
      "['SOS', 'Nicos', 'Anastasiades', 'EOS']\n",
      "Adding word 50000 to our vocabulary.\n",
      "['SOS', 'ʻAsim', 's', 'EOS']\n",
      "Adding word 51000 to our vocabulary.\n",
      "['SOS', 'When', 'was', 'the', 'Marzpanate', 'era', 'EOS']\n",
      "Adding word 52000 to our vocabulary.\n",
      "['SOS', 'Who', 'invaded', 'the', 'Archaemenid', 'Empire', 'in', '334BC', 'EOS']\n",
      "Adding word 53000 to our vocabulary.\n",
      "['SOS', 'What', 'church', 'deacon', 'was', 'washed', 'ashore', 'on', 'Nukulaelae', 'EOS']\n",
      "Adding word 54000 to our vocabulary.\n",
      "['SOS', 'Hathor', 'EOS']\n",
      "Adding word 55000 to our vocabulary.\n",
      "['SOS', 'When', 'did', 'Xudong', 'An', 'and', 'Anthony', 'B', 'Sanders', 'issue', 'a', 'report', 'about', 'commercial', 'mortgage', 'backed', 'securities', 'EOS']\n",
      "Adding word 56000 to our vocabulary.\n",
      "['SOS', 'Drangiana', 'Arachosia', 'Gedrosia', 'and', 'Seistan', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "myVocab = Vocab(name = \"myVocab\")\n",
    "count = 0\n",
    "for titleIdx, paragraphs in df.iterrows():\n",
    "    for para in paragraphs['paragraphs']:\n",
    "        for qa in para['qas']:\n",
    "            text = prepare_text(qa['question'])\n",
    "            for t in text:\n",
    "                if (myVocab.indexWord(t.lower())):\n",
    "                    if (count % 1000) == 0:\n",
    "                        print(\"Adding word {} to our vocabulary.\".format(count))\n",
    "                        print(text)\n",
    "                    count += 1\n",
    "            \n",
    "            # print(qa['answers'][0])\n",
    "            # ansList = []\n",
    "            for answer in qa['answers']:\n",
    "                text = prepare_text(answer['text'])\n",
    "                # ansList.append(text)\n",
    "                for t in text:\n",
    "                    if (myVocab.indexWord(t.lower())):\n",
    "                        if (count % 1000) == 0:\n",
    "                            print(\"Adding word {} to our vocabulary.\".format(count))\n",
    "                            print(text)\n",
    "                        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sos': 217148,\n",
       " 'when': 9991,\n",
       " 'did': 23178,\n",
       " 'beyonce': 307,\n",
       " 'start': 692,\n",
       " 'becoming': 115,\n",
       " 'popular': 500,\n",
       " 'eos': 217140,\n",
       " 'in': 42036,\n",
       " 'the': 107784,\n",
       " 'late': 452,\n",
       " '1990s': 128,\n",
       " 'what': 78062,\n",
       " 'areas': 548,\n",
       " 'compete': 72,\n",
       " 'she': 227,\n",
       " 'was': 25286,\n",
       " 'growing': 107,\n",
       " 'up': 1225,\n",
       " 'singing': 41,\n",
       " 'and': 18513,\n",
       " 'dancing': 13,\n",
       " 'leave': 195,\n",
       " 'destiny': 50,\n",
       " 's': 13589,\n",
       " 'child': 306,\n",
       " 'become': 1143,\n",
       " 'a': 20229,\n",
       " 'solo': 37,\n",
       " 'singer': 72,\n",
       " '2003': 239,\n",
       " 'city': 2640,\n",
       " 'state': 1826,\n",
       " 'grow': 116,\n",
       " 'houston': 327,\n",
       " 'texas': 211,\n",
       " 'which': 8974,\n",
       " 'decade': 261,\n",
       " 'famous': 360,\n",
       " 'r': 157,\n",
       " 'b': 214,\n",
       " 'group': 1615,\n",
       " 'lead': 331,\n",
       " 'album': 303,\n",
       " 'made': 1400,\n",
       " 'her': 578,\n",
       " 'worldwide': 146,\n",
       " 'known': 1387,\n",
       " 'artist': 163,\n",
       " 'dangerously': 13,\n",
       " 'love': 114,\n",
       " 'who': 14229,\n",
       " 'managed': 45,\n",
       " 'mathew': 2,\n",
       " 'knowles': 1,\n",
       " 'beyoncé': 232,\n",
       " 'rise': 197,\n",
       " 'to': 31128,\n",
       " 'fame': 41,\n",
       " 'role': 334,\n",
       " 'have': 4880,\n",
       " 'first': 4142,\n",
       " 'released': 558,\n",
       " 'as': 6384,\n",
       " 'release': 331,\n",
       " 'how': 13557,\n",
       " 'many': 7984,\n",
       " 'grammy': 36,\n",
       " 'awards': 108,\n",
       " 'win': 318,\n",
       " 'for': 11973,\n",
       " 'five': 486,\n",
       " 'name': 3721,\n",
       " 'of': 58110,\n",
       " 'after': 2240,\n",
       " 'second': 913,\n",
       " 'other': 2498,\n",
       " 'entertainment': 109,\n",
       " 'venture': 24,\n",
       " 'explore': 27,\n",
       " 'acting': 30,\n",
       " 'marry': 72,\n",
       " 'jay': 45,\n",
       " 'z': 51,\n",
       " 'set': 416,\n",
       " 'record': 340,\n",
       " 'grammys': 6,\n",
       " 'six': 276,\n",
       " 'movie': 274,\n",
       " 'receive': 338,\n",
       " 'golden': 108,\n",
       " 'globe': 35,\n",
       " 'nomination': 14,\n",
       " 'dreamgirls': 11,\n",
       " 'take': 984,\n",
       " 'hiatus': 13,\n",
       " 'career': 89,\n",
       " 'control': 602,\n",
       " 'management': 145,\n",
       " '2010': 601,\n",
       " 'darker': 28,\n",
       " 'tone': 29,\n",
       " 'from': 6705,\n",
       " 'previous': 123,\n",
       " 'work': 1029,\n",
       " 'portraying': 5,\n",
       " 'etta': 5,\n",
       " 'james': 294,\n",
       " 'create': 381,\n",
       " 'sasha': 17,\n",
       " 'fierce': 16,\n",
       " 'cadillac': 8,\n",
       " 'records': 346,\n",
       " 'end': 906,\n",
       " 'their': 2641,\n",
       " 'act': 745,\n",
       " 'june': 353,\n",
       " '2005': 384,\n",
       " 'day': 801,\n",
       " 'job': 122,\n",
       " '2006': 440,\n",
       " 'is': 25185,\n",
       " 'married': 82,\n",
       " 'alter': 28,\n",
       " 'ego': 6,\n",
       " 'music': 1121,\n",
       " 'are': 9040,\n",
       " 'some': 1411,\n",
       " 'recurring': 6,\n",
       " 'elements': 175,\n",
       " 'them': 385,\n",
       " 'relationships': 75,\n",
       " 'monogamy': 1,\n",
       " 'time': 1812,\n",
       " 'magazine': 190,\n",
       " 'named': 385,\n",
       " 'one': 2695,\n",
       " 'most': 2611,\n",
       " '100': 304,\n",
       " 'people': 2631,\n",
       " 'century': 2274,\n",
       " 'influential': 100,\n",
       " 'declared': 153,\n",
       " 'dominant': 115,\n",
       " 'woman': 116,\n",
       " 'musician': 29,\n",
       " 'forbes': 49,\n",
       " 'recording': 113,\n",
       " 'industry': 357,\n",
       " 'association': 247,\n",
       " 'america': 491,\n",
       " 'recognize': 75,\n",
       " 'top': 382,\n",
       " 'certified': 16,\n",
       " '2000s': 55,\n",
       " 'rated': 52,\n",
       " 'powerful': 56,\n",
       " 'female': 188,\n",
       " '2015': 396,\n",
       " 'describe': 326,\n",
       " 'herself': 21,\n",
       " 'feminist': 19,\n",
       " 'modern': 733,\n",
       " 'years': 1360,\n",
       " 'rate': 436,\n",
       " 'world': 1896,\n",
       " '2013': 506,\n",
       " '2014': 445,\n",
       " 'has': 3403,\n",
       " 'sold': 213,\n",
       " '19': 158,\n",
       " 'year': 5697,\n",
       " '118': 12,\n",
       " 'million': 1083,\n",
       " 'sell': 197,\n",
       " 'part': 1439,\n",
       " '60': 178,\n",
       " 'leaving': 70,\n",
       " 'under': 861,\n",
       " 'own': 370,\n",
       " 'won': 361,\n",
       " '20': 382,\n",
       " 'younger': 44,\n",
       " 'sibling': 7,\n",
       " 'also': 720,\n",
       " 'sang': 23,\n",
       " 'with': 6970,\n",
       " 'band': 181,\n",
       " 'where': 5637,\n",
       " 'get': 630,\n",
       " 'mother': 229,\n",
       " 'maiden': 8,\n",
       " 'race': 282,\n",
       " 'father': 285,\n",
       " 'african': 400,\n",
       " 'american': 1369,\n",
       " 'childhood': 44,\n",
       " 'home': 448,\n",
       " 'believed': 285,\n",
       " 'religion': 552,\n",
       " 'methodist': 33,\n",
       " 'worked': 112,\n",
       " 'sales': 153,\n",
       " 'manager': 83,\n",
       " 'company': 1078,\n",
       " 'xerox': 5,\n",
       " 'hairdresser': 2,\n",
       " 'salon': 26,\n",
       " 'owner': 51,\n",
       " 'sister': 70,\n",
       " 'appeared': 86,\n",
       " 'solange': 4,\n",
       " 'descendent': 1,\n",
       " 'arcadian': 1,\n",
       " 'leader': 404,\n",
       " 'joseph': 114,\n",
       " 'broussard': 2,\n",
       " 'descendant': 11,\n",
       " 'acadian': 2,\n",
       " 'raised': 91,\n",
       " 'town': 268,\n",
       " 'go': 408,\n",
       " 'school': 1216,\n",
       " 'fredericksburg': 2,\n",
       " 'person': 442,\n",
       " 'notice': 30,\n",
       " 'ability': 147,\n",
       " 'darlette': 3,\n",
       " 'johnson': 88,\n",
       " 'moved': 155,\n",
       " 'left': 270,\n",
       " 'elementary': 33,\n",
       " 'teachers': 53,\n",
       " 'discovered': 283,\n",
       " 'musical': 123,\n",
       " 'talent': 12,\n",
       " 'dance': 141,\n",
       " 'instructor': 6,\n",
       " 'i': 584,\n",
       " 'church': 1054,\n",
       " 'member': 270,\n",
       " 'soloist': 1,\n",
       " 'choir': 16,\n",
       " 'st': 492,\n",
       " 'john': 908,\n",
       " 'united': 1371,\n",
       " 'type': 3109,\n",
       " 'parker': 7,\n",
       " 'magnet': 7,\n",
       " 'song': 355,\n",
       " 'sing': 40,\n",
       " 'competition': 197,\n",
       " 'at': 4023,\n",
       " 'age': 619,\n",
       " '7': 406,\n",
       " 'imagine': 8,\n",
       " 'located': 1361,\n",
       " 'old': 658,\n",
       " 'show': 472,\n",
       " 'seven': 246,\n",
       " 'two': 2485,\n",
       " 'decided': 120,\n",
       " 'place': 884,\n",
       " 'star': 144,\n",
       " 'search': 57,\n",
       " 'arne': 4,\n",
       " 'frager': 3,\n",
       " '1995': 125,\n",
       " 'manage': 43,\n",
       " 'girls': 63,\n",
       " 'label': 116,\n",
       " 'give': 368,\n",
       " 'deal': 173,\n",
       " 'elektra': 4,\n",
       " 'brought': 242,\n",
       " 'california': 122,\n",
       " 'enter': 114,\n",
       " 'quit': 16,\n",
       " 'his': 2089,\n",
       " 'large': 694,\n",
       " 'recorded': 160,\n",
       " 'sony': 172,\n",
       " 'signed': 181,\n",
       " 'later': 310,\n",
       " 'cut': 96,\n",
       " 'meet': 171,\n",
       " 'latavia': 2,\n",
       " 'robertson': 5,\n",
       " 'eight': 186,\n",
       " 'met': 83,\n",
       " 'roberson': 4,\n",
       " 'girl': 23,\n",
       " 'tyme': 2,\n",
       " 'placed': 118,\n",
       " 'begin': 945,\n",
       " 'on': 7783,\n",
       " 'october': 299,\n",
       " '5': 715,\n",
       " 'dwayne': 4,\n",
       " 'wiggins': 1,\n",
       " 'grass': 21,\n",
       " 'roots': 70,\n",
       " 'film': 524,\n",
       " 'featured': 146,\n",
       " 'major': 738,\n",
       " 'single': 405,\n",
       " 'men': 345,\n",
       " 'black': 480,\n",
       " 'award': 238,\n",
       " 'best': 339,\n",
       " 'performance': 136,\n",
       " 'say': 496,\n",
       " 'my': 59,\n",
       " 'man': 278,\n",
       " 'marc': 6,\n",
       " 'nelson': 34,\n",
       " 'changed': 171,\n",
       " '1996': 106,\n",
       " 'based': 789,\n",
       " 'quote': 24,\n",
       " 'book': 680,\n",
       " 'bible': 140,\n",
       " 'isaiah': 10,\n",
       " 'debut': 56,\n",
       " 'killing': 48,\n",
       " 'sound': 233,\n",
       " 'track': 59,\n",
       " '43': 44,\n",
       " 'annual': 162,\n",
       " 'included': 296,\n",
       " 'soundtrack': 26,\n",
       " 'hit': 169,\n",
       " 'no': 910,\n",
       " '1999': 124,\n",
       " 'duet': 6,\n",
       " 'mental': 56,\n",
       " 'health': 286,\n",
       " 'issue': 243,\n",
       " 'through': 671,\n",
       " 'depression': 59,\n",
       " 'event': 521,\n",
       " 'occured': 25,\n",
       " 'publicly': 32,\n",
       " 'criticized': 101,\n",
       " 'boyfriend': 2,\n",
       " 'supported': 206,\n",
       " 'caused': 574,\n",
       " 'split': 112,\n",
       " 'luckett': 4,\n",
       " 'rober': 4,\n",
       " 'long': 1293,\n",
       " 'depressed': 4,\n",
       " 'couple': 18,\n",
       " 'helped': 296,\n",
       " 'fight': 137,\n",
       " 'replaced': 263,\n",
       " 'farrah': 3,\n",
       " 'franklin': 58,\n",
       " 'michelle': 8,\n",
       " 'williams': 68,\n",
       " 'blamed': 23,\n",
       " 'overcome': 27,\n",
       " 'during': 2551,\n",
       " 'following': 198,\n",
       " 'newest': 27,\n",
       " 'removed': 183,\n",
       " 'charlie': 6,\n",
       " 'angels': 12,\n",
       " 'members': 581,\n",
       " 'independent': 212,\n",
       " 'women': 422,\n",
       " 'weeks': 71,\n",
       " 'stay': 101,\n",
       " 'eleven': 34,\n",
       " 'network': 296,\n",
       " 'land': 516,\n",
       " 'mtv': 46,\n",
       " 'third': 487,\n",
       " 'survivor': 14,\n",
       " 'its': 1954,\n",
       " 'week': 130,\n",
       " '663': 3,\n",
       " '000': 1353,\n",
       " 'copies': 98,\n",
       " 'french': 909,\n",
       " 'composer': 34,\n",
       " 'wrote': 513,\n",
       " 'original': 426,\n",
       " 'opera': 104,\n",
       " 'carmen': 4,\n",
       " '19th': 412,\n",
       " 'georges': 10,\n",
       " 'bizet': 2,\n",
       " 'lawsuit': 40,\n",
       " 'be': 4990,\n",
       " 'filed': 59,\n",
       " '2001': 244,\n",
       " '2000': 319,\n",
       " 'mekhi': 1,\n",
       " 'phifer': 6,\n",
       " 'hip': 44,\n",
       " 'hopera': 1,\n",
       " 'over': 1344,\n",
       " 'announce': 124,\n",
       " 'austin': 17,\n",
       " 'powers': 253,\n",
       " 'goldmember': 6,\n",
       " 'mike': 27,\n",
       " 'myers': 5,\n",
       " 'three': 1128,\n",
       " 'countries': 899,\n",
       " 'it': 2691,\n",
       " 'out': 903,\n",
       " 'achieve': 119,\n",
       " 'ten': 175,\n",
       " 'status': 263,\n",
       " 'uk': 439,\n",
       " 'norway': 32,\n",
       " 'belgium': 39,\n",
       " 'starred': 50,\n",
       " 'cuba': 36,\n",
       " 'gooding': 3,\n",
       " 'jr': 76,\n",
       " 'fighting': 114,\n",
       " 'temptations': 7,\n",
       " 'missy': 2,\n",
       " 'elliott': 12,\n",
       " 'better': 170,\n",
       " 'charts': 44,\n",
       " 'summertime': 7,\n",
       " 'appear': 241,\n",
       " 'amount': 352,\n",
       " 'gross': 45,\n",
       " '73': 23,\n",
       " 'genre': 134,\n",
       " 'comedy': 40,\n",
       " 'critics': 91,\n",
       " 'view': 299,\n",
       " 'mixed': 136,\n",
       " 'reviews': 22,\n",
       " '2002': 207,\n",
       " 'character': 223,\n",
       " 'called': 1801,\n",
       " 'foxxy': 1,\n",
       " 'cleopatra': 11,\n",
       " 'along': 961,\n",
       " 'tempations': 1,\n",
       " 'highest': 383,\n",
       " 'achieved': 62,\n",
       " 'billboard': 36,\n",
       " 'hot': 113,\n",
       " 'number': 695,\n",
       " 'four': 681,\n",
       " 'by': 6838,\n",
       " 'sould': 2,\n",
       " 'since': 597,\n",
       " '11': 279,\n",
       " 'crazy': 16,\n",
       " 'singles': 26,\n",
       " 'came': 311,\n",
       " 'u': 708,\n",
       " 'spot': 35,\n",
       " 'chart': 39,\n",
       " 'closer': 34,\n",
       " 'you': 449,\n",
       " 'luther': 51,\n",
       " 'vandross': 4,\n",
       " 'associated': 318,\n",
       " 'premiere': 26,\n",
       " '24': 176,\n",
       " 'earn': 72,\n",
       " 'duo': 6,\n",
       " 'or': 2871,\n",
       " '46th': 2,\n",
       " 'final': 289,\n",
       " 'fulfilled': 8,\n",
       " 'got': 72,\n",
       " 'hollywood': 42,\n",
       " 'walk': 27,\n",
       " 'embark': 9,\n",
       " 'tour': 116,\n",
       " 'europe': 629,\n",
       " 'november': 272,\n",
       " 'announced': 129,\n",
       " 'that': 6925,\n",
       " 'would': 1266,\n",
       " 'disban': 1,\n",
       " 'european': 746,\n",
       " 'barcelona': 181,\n",
       " 'march': 373,\n",
       " 'started': 303,\n",
       " 'verizon': 3,\n",
       " 'lades': 1,\n",
       " 'alicia': 2,\n",
       " 'keys': 12,\n",
       " 'perform': 149,\n",
       " 'february': 222,\n",
       " '1': 1160,\n",
       " '2004': 301,\n",
       " 'super': 114,\n",
       " 'bowl': 66,\n",
       " 'xxxviii': 1,\n",
       " 'studio': 72,\n",
       " 'albums': 55,\n",
       " '541': 7,\n",
       " 'déjà': 2,\n",
       " 'vu': 6,\n",
       " 'irreplaceable': 1,\n",
       " 'produce': 322,\n",
       " 'birthday': 24,\n",
       " 'celebrate': 52,\n",
       " 'twenty': 62,\n",
       " 'fifth': 114,\n",
       " 'deja': 4,\n",
       " 'high': 949,\n",
       " 'climb': 10,\n",
       " 'collaborated': 12,\n",
       " 'only': 1083,\n",
       " 'k': 49,\n",
       " 'green': 283,\n",
       " 'light': 572,\n",
       " 'pink': 18,\n",
       " 'panther': 13,\n",
       " 'listen': 10,\n",
       " '2007': 534,\n",
       " 'much': 1885,\n",
       " 'money': 351,\n",
       " 'make': 934,\n",
       " 'millions': 45,\n",
       " 'dollars': 64,\n",
       " 'wide': 112,\n",
       " '158': 8,\n",
       " '8': 517,\n",
       " 'call': 383,\n",
       " 'concert': 62,\n",
       " 'experience': 201,\n",
       " 'beautiful': 46,\n",
       " 'liar': 3,\n",
       " 'shakira': 3,\n",
       " 'steve': 42,\n",
       " 'martin': 98,\n",
       " 'pop': 109,\n",
       " 'diana': 36,\n",
       " 'ross': 18,\n",
       " 'international': 688,\n",
       " '2008': 532,\n",
       " 'whom': 468,\n",
       " 'am': 39,\n",
       " '18': 203,\n",
       " 'more': 1735,\n",
       " 'songs': 134,\n",
       " 'than': 1573,\n",
       " 'any': 445,\n",
       " 'beat': 59,\n",
       " 'video': 293,\n",
       " 'taylor': 60,\n",
       " 'swift': 23,\n",
       " '2009': 525,\n",
       " 'grossed': 13,\n",
       " '119': 10,\n",
       " 'reveal': 42,\n",
       " 'marriage': 101,\n",
       " 'montage': 1,\n",
       " 'april': 296,\n",
       " '4': 559,\n",
       " 'ladies': 23,\n",
       " 'prominent': 128,\n",
       " 'felt': 58,\n",
       " 'should': 391,\n",
       " 'went': 110,\n",
       " 'instead': 222,\n",
       " 'kanye': 357,\n",
       " 'west': 809,\n",
       " 'portrayed': 34,\n",
       " 'gave': 217,\n",
       " 'entire': 102,\n",
       " 'salary': 20,\n",
       " 'organization': 543,\n",
       " 'phoenix': 15,\n",
       " 'house': 793,\n",
       " 'inaugural': 21,\n",
       " 'ball': 68,\n",
       " 'last': 818,\n",
       " 'obsessed': 7,\n",
       " 'thriller': 5,\n",
       " 'scene': 65,\n",
       " 'donate': 38,\n",
       " 'played': 316,\n",
       " 'sharon': 3,\n",
       " 'charles': 323,\n",
       " 'buget': 1,\n",
       " 'portray': 16,\n",
       " 'received': 189,\n",
       " 'january': 332,\n",
       " 'ali': 63,\n",
       " 'larter': 1,\n",
       " 'nominated': 32,\n",
       " '52nd': 6,\n",
       " 'tied': 30,\n",
       " 'nominations': 23,\n",
       " 'lauryn': 3,\n",
       " 'hill': 97,\n",
       " 'lady': 46,\n",
       " 'gaga': 7,\n",
       " 'now': 345,\n",
       " 'telephone': 62,\n",
       " 'mariah': 11,\n",
       " 'carey': 13,\n",
       " 'sixth': 76,\n",
       " 'else': 325,\n",
       " 'they': 1151,\n",
       " 'tie': 24,\n",
       " 'ceremony': 60,\n",
       " 'artists': 140,\n",
       " 'hits': 30,\n",
       " '1992': 130,\n",
       " 'break': 112,\n",
       " 'business': 349,\n",
       " 'ways': 136,\n",
       " 'landmark': 44,\n",
       " 'see': 285,\n",
       " 'china': 664,\n",
       " 'great': 695,\n",
       " 'wall': 109,\n",
       " 'inspired': 97,\n",
       " 'this': 1174,\n",
       " 'stop': 303,\n",
       " 'using': 534,\n",
       " 'nine': 135,\n",
       " 'months': 186,\n",
       " 'suggested': 109,\n",
       " 'reports': 53,\n",
       " 'about': 1859,\n",
       " 'performing': 49,\n",
       " 'muammar': 8,\n",
       " 'gaddafi': 247,\n",
       " 'surface': 157,\n",
       " '2011': 497,\n",
       " 'earned': 35,\n",
       " 'shows': 118,\n",
       " 'clinton': 50,\n",
       " 'bush': 122,\n",
       " 'haiti': 11,\n",
       " 'fund': 108,\n",
       " 'became': 545,\n",
       " 'stage': 134,\n",
       " 'glastonbury': 19,\n",
       " 'festival': 247,\n",
       " 'spokespeople': 1,\n",
       " 'confirm': 24,\n",
       " 'donations': 27,\n",
       " 'huffington': 3,\n",
       " 'post': 346,\n",
       " 'listed': 116,\n",
       " 'paid': 101,\n",
       " 'performer': 7,\n",
       " 'per': 468,\n",
       " 'minute': 34,\n",
       " 'hoe': 6,\n",
       " 'everyone': 41,\n",
       " 'learn': 83,\n",
       " 'performed': 128,\n",
       " 'kaddafi': 1,\n",
       " 'documents': 80,\n",
       " 'obtained': 37,\n",
       " 'wikileaks': 2,\n",
       " 'leak': 5,\n",
       " 'happen': 215,\n",
       " 'tell': 63,\n",
       " 'donation': 15,\n",
       " 'privately': 18,\n",
       " 'information': 338,\n",
       " 'libyan': 45,\n",
       " 'ruler': 122,\n",
       " 'pay': 212,\n",
       " 'private': 304,\n",
       " 'headline': 11,\n",
       " 'pyramid': 21,\n",
       " 'fourth': 173,\n",
       " 'debuted': 17,\n",
       " 'had': 1688,\n",
       " 'success': 129,\n",
       " 'an': 4243,\n",
       " 'activity': 154,\n",
       " 'writing': 177,\n",
       " 'nights': 41,\n",
       " 'new': 2970,\n",
       " 'york': 632,\n",
       " 'roseland': 3,\n",
       " 'ballroom': 25,\n",
       " 'forth': 24,\n",
       " '28': 127,\n",
       " '310': 9,\n",
       " 'awarded': 97,\n",
       " 'journalists': 22,\n",
       " 'write': 328,\n",
       " 'story': 115,\n",
       " 'earlier': 103,\n",
       " 'essence': 20,\n",
       " 'standing': 58,\n",
       " 'room': 100,\n",
       " 'concerts': 32,\n",
       " '2012': 501,\n",
       " 'birth': 136,\n",
       " 'lenox': 6,\n",
       " 'hospital': 131,\n",
       " 'blue': 197,\n",
       " 'ivy': 13,\n",
       " 'carter': 17,\n",
       " 'appearance': 77,\n",
       " 'giving': 58,\n",
       " 'revel': 8,\n",
       " 'atlantic': 234,\n",
       " 'ovation': 3,\n",
       " 'hall': 208,\n",
       " 'before': 881,\n",
       " 'again': 97,\n",
       " 'daughter': 61,\n",
       " 'born': 342,\n",
       " 'public': 785,\n",
       " 'play': 484,\n",
       " 'resort': 29,\n",
       " 'compilation': 15,\n",
       " 'topic': 58,\n",
       " 'romance': 66,\n",
       " 'documentary': 29,\n",
       " 'life': 565,\n",
       " 'but': 362,\n",
       " 'dream': 16,\n",
       " 'sign': 185,\n",
       " 'global': 175,\n",
       " 'publishing': 62,\n",
       " 'agreement': 281,\n",
       " 'title': 429,\n",
       " 'added': 229,\n",
       " 'nuclear': 158,\n",
       " 'whose': 513,\n",
       " 'inauguration': 10,\n",
       " 'national': 980,\n",
       " 'anthem': 15,\n",
       " 'president': 845,\n",
       " 'obama': 63,\n",
       " 'tweets': 3,\n",
       " 'half': 261,\n",
       " '268': 1,\n",
       " 'month': 350,\n",
       " 'xlvii': 1,\n",
       " 'halftime': 1,\n",
       " 'dates': 71,\n",
       " 'mrs': 14,\n",
       " 'entail': 10,\n",
       " '132': 9,\n",
       " 'successful': 161,\n",
       " 'tours': 19,\n",
       " 'yet': 50,\n",
       " 'epic': 41,\n",
       " 'voiced': 38,\n",
       " 'animated': 6,\n",
       " '15': 362,\n",
       " 'honorary': 16,\n",
       " 'chair': 44,\n",
       " 'gala': 4,\n",
       " 'voice': 68,\n",
       " 'queen': 625,\n",
       " 'tara': 2,\n",
       " 'amy': 8,\n",
       " 'winehouse': 1,\n",
       " 'cover': 161,\n",
       " 'may': 896,\n",
       " 'back': 398,\n",
       " '5th': 76,\n",
       " 'huge': 38,\n",
       " 'surprise': 11,\n",
       " 'itunes': 50,\n",
       " 'store': 285,\n",
       " 'consecutive': 37,\n",
       " 'december': 315,\n",
       " '13': 217,\n",
       " 'joined': 92,\n",
       " 'run': 302,\n",
       " 'reported': 157,\n",
       " 'e': 211,\n",
       " 'earning': 14,\n",
       " 'were': 6589,\n",
       " 'earnings': 19,\n",
       " 'double': 103,\n",
       " 'digital': 187,\n",
       " 'days': 312,\n",
       " 'husband': 24,\n",
       " 'drunk': 1,\n",
       " 'featuring': 18,\n",
       " 'both': 440,\n",
       " '57th': 9,\n",
       " 'beck': 8,\n",
       " 'pose': 9,\n",
       " 'august': 262,\n",
       " 'vogue': 15,\n",
       " 'superbowl': 4,\n",
       " '50': 209,\n",
       " 'coldplay': 6,\n",
       " 'took': 348,\n",
       " 'lost': 256,\n",
       " 'next': 138,\n",
       " 'if': 773,\n",
       " 'grammies': 4,\n",
       " 'model': 409,\n",
       " 'british': 1089,\n",
       " 'entertainer': 6,\n",
       " 'making': 187,\n",
       " 'do': 5015,\n",
       " 'so': 267,\n",
       " 'formation': 111,\n",
       " 'online': 108,\n",
       " 'service': 564,\n",
       " 'tidal': 28,\n",
       " '6': 446,\n",
       " '2016': 117,\n",
       " 'exclusively': 36,\n",
       " 'streaming': 21,\n",
       " 'kind': 1300,\n",
       " 'platform': 61,\n",
       " 'together': 187,\n",
       " '300': 134,\n",
       " 'pregnant': 20,\n",
       " 'paris': 382,\n",
       " 'described': 183,\n",
       " 'hardest': 7,\n",
       " 'thing': 140,\n",
       " 'endure': 10,\n",
       " 'miscarriage': 3,\n",
       " 'relationship': 255,\n",
       " 'girlfriend': 5,\n",
       " 'creating': 98,\n",
       " 'speculation': 11,\n",
       " '03': 8,\n",
       " 'bonnie': 3,\n",
       " 'clyde': 2,\n",
       " 'combined': 118,\n",
       " 'saddest': 1,\n",
       " 'attended': 94,\n",
       " 'confirmed': 31,\n",
       " 'pregnancy': 22,\n",
       " 'watched': 36,\n",
       " '12': 383,\n",
       " 'why': 1948,\n",
       " 'broadcast': 178,\n",
       " 'history': 431,\n",
       " 'even': 182,\n",
       " 'guinness': 21,\n",
       " 'searched': 3,\n",
       " 'term': 1469,\n",
       " 'aug': 5,\n",
       " '29': 95,\n",
       " 'prior': 288,\n",
       " 'announcing': 7,\n",
       " 'google': 41,\n",
       " 'website': 90,\n",
       " 'lifeandtimes': 1,\n",
       " 'com': 43,\n",
       " 'talked': 16,\n",
       " 'struggles': 8,\n",
       " 'glory': 8,\n",
       " 'baby': 11,\n",
       " 'delivered': 33,\n",
       " 'dedicated': 85,\n",
       " 'does': 7020,\n",
       " 'c': 348,\n",
       " 'stand': 408,\n",
       " 'credited': 88,\n",
       " 'cries': 3,\n",
       " 'rally': 27,\n",
       " 'acquittal': 1,\n",
       " 'george': 400,\n",
       " 'zimmerman': 2,\n",
       " 'presidential': 160,\n",
       " 'raise': 58,\n",
       " '40': 241,\n",
       " 'club': 337,\n",
       " 'endorse': 20,\n",
       " '26': 132,\n",
       " 'same': 466,\n",
       " 'sex': 103,\n",
       " 'attend': 166,\n",
       " 'july': 308,\n",
       " 'obamas': 1,\n",
       " 'social': 471,\n",
       " 'media': 276,\n",
       " 'upload': 10,\n",
       " 'picture': 64,\n",
       " 'paper': 333,\n",
       " 'ballot': 16,\n",
       " 'tumblr': 1,\n",
       " 'interview': 19,\n",
       " 'asked': 67,\n",
       " 'feminism': 4,\n",
       " 'campaign': 139,\n",
       " 'encourages': 20,\n",
       " 'leadership': 80,\n",
       " 'ban': 106,\n",
       " 'bossy': 4,\n",
       " 'quoted': 27,\n",
       " 'saying': 36,\n",
       " 'contribute': 76,\n",
       " 'response': 154,\n",
       " 'speech': 152,\n",
       " 'flawless': 2,\n",
       " 'encourage': 51,\n",
       " 'used': 3346,\n",
       " 'words': 222,\n",
       " 'nigerian': 52,\n",
       " 'author': 163,\n",
       " 'chimamanda': 2,\n",
       " 'ngozi': 2,\n",
       " 'adichie': 2,\n",
       " 'females': 51,\n",
       " 'letter': 184,\n",
       " 'important': 542,\n",
       " 'un': 189,\n",
       " 'summit': 23,\n",
       " 'september': 326,\n",
       " 'focused': 82,\n",
       " 'developing': 106,\n",
       " 'funding': 109,\n",
       " 'priorities': 7,\n",
       " 'addressed': 27,\n",
       " 'angela': 8,\n",
       " 'merkel': 4,\n",
       " 'nkosazana': 2,\n",
       " 'dlamini': 3,\n",
       " 'zuma': 3,\n",
       " 'these': 454,\n",
       " 'head': 259,\n",
       " 'g7': 4,\n",
       " 'germany': 510,\n",
       " 'will': 658,\n",
       " 'serving': 70,\n",
       " 'relation': 110,\n",
       " 'want': 359,\n",
       " 'recipients': 7,\n",
       " 'focus': 267,\n",
       " 'family': 391,\n",
       " 'death': 542,\n",
       " 'freddie': 43,\n",
       " 'gray': 54,\n",
       " 'lots': 16,\n",
       " 'bail': 8,\n",
       " 'prison': 36,\n",
       " 'protesters': 54,\n",
       " 'protest': 71,\n",
       " 'spend': 114,\n",
       " 'thousands': 76,\n",
       " 'between': 1767,\n",
       " 'madonna': 272,\n",
       " 'celine': 4,\n",
       " 'dion': 4,\n",
       " 'power': 1036,\n",
       " 'until': 388,\n",
       " 'total': 305,\n",
       " 'worth': 112,\n",
       " '250': 64,\n",
       " 'entertainers': 2,\n",
       " '16': 267,\n",
       " 'celebrity': 19,\n",
       " 'list': 175,\n",
       " '115': 14,\n",
       " 'net': 48,\n",
       " 'began': 357,\n",
       " 'reporting': 25,\n",
       " 'starting': 101,\n",
       " 'ever': 115,\n",
       " 'predicted': 32,\n",
       " 'billion': 351,\n",
       " 'range': 322,\n",
       " 'octaves': 8,\n",
       " 'timbre': 1,\n",
       " 'distinctive': 27,\n",
       " 'jody': 4,\n",
       " 'rosen': 8,\n",
       " 'critic': 40,\n",
       " 'versatile': 5,\n",
       " 'daily': 157,\n",
       " 'mail': 44,\n",
       " 'era': 466,\n",
       " 'influenced': 276,\n",
       " 'style': 519,\n",
       " 'hop': 44,\n",
       " 'claim': 235,\n",
       " 'praise': 22,\n",
       " 'span': 62,\n",
       " 'centerpiece': 3,\n",
       " 'vocal': 45,\n",
       " 'abilities': 20,\n",
       " 'times': 730,\n",
       " 'jon': 17,\n",
       " 'pareles': 1,\n",
       " 'calls': 51,\n",
       " 'velvety': 1,\n",
       " 'tart': 2,\n",
       " 'generally': 224,\n",
       " 'categorized': 23,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocab.words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56523\n",
      "2863\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(myVocab.words))\n",
    "\n",
    "threshold = 1e-1 # test 10, 1, 1e-1, 1e-2\n",
    "p_disc = {myVocab.index[str(idx)]: 1 - np.sqrt(threshold / myVocab.words_freq[myVocab.index[str(idx)]]) for idx in range(myVocab.count)}\n",
    "\n",
    "trainVocab = Vocab(name = \"trainVocab\")\n",
    "for i in range(myVocab.count):\n",
    "    word = myVocab.index[str(i)]\n",
    "    word_freq = myVocab.words_freq[word]\n",
    "    idx = myVocab.words[word]\n",
    "    if (random.random() < (1-p_disc[word]) and word_freq > 2):\n",
    "        trainVocab.indexWord(word)\n",
    "\n",
    "\n",
    "trainVocab.indexWord('sos')\n",
    "trainVocab.indexWord('eos')\n",
    "print(len(trainVocab.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86821"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QDataset(Dataset):\n",
    "    \"\"\" Q dataset \"\"\"\n",
    "    def __init__(self, dataFrame):\n",
    "        self.questions = []\n",
    "        for titleIdx, paragraphs in dataFrame.iterrows():\n",
    "            for para in paragraphs['paragraphs']:\n",
    "                for qa in para['qas']:\n",
    "                    text = prepare_text(qa['question'])\n",
    "                    question = []\n",
    "                    if len(qa['answers']) > 0:\n",
    "                        for t in text:\n",
    "                            try:\n",
    "                                question.append(trainVocab.words[t.lower()])\n",
    "                            except:\n",
    "                                question.append(len(trainVocab.words))\n",
    "                        self.questions.append(question)\n",
    "\n",
    "\n",
    "        # self.questions = torch.Tensor(self.questions)\n",
    "\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        return self.questions[idx]\n",
    "\n",
    "class ADataset(Dataset):\n",
    "    \"\"\" A dataset \"\"\"\n",
    "    def __init__(self, dataFrame):\n",
    "        self.answers = []\n",
    "        for titleIdx, paragraphs in dataFrame.iterrows():\n",
    "            for para in paragraphs['paragraphs']:\n",
    "                for qa in para['qas']:\n",
    "                    ansList = []\n",
    "                    for answer in qa['answers']:\n",
    "                        answerVocab = []\n",
    "                        text = prepare_text(answer['text'])\n",
    "                        for t in text:\n",
    "                            try:\n",
    "                                answerVocab.append(trainVocab.words[t.lower()])\n",
    "                            except:\n",
    "                                answerVocab.append(len(trainVocab.words))\n",
    "                        ansList.append(answerVocab)\n",
    "\n",
    "                    if len(ansList) > 0:\n",
    "                        self.answers.append(ansList[0])\n",
    "\n",
    "        # self.answers = torch.Tensor(self.answers)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.answers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        return self.answers[idx]\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, qDataset, aDataset):\n",
    "        self.src = qDataset\n",
    "        self.trg = aDataset\n",
    "        assert(len(self.src) == len(self.trg))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        return {\"question\": torch.tensor(self.src[idx]), \n",
    "                \"answer\": torch.tensor(self.trg[idx])}\n",
    "        \n",
    "\n",
    "\n",
    "SRC_dataset = QDataset(df)\n",
    "TRG_dataset = ADataset(df)\n",
    "\n",
    "dataset = QADataset(SRC_dataset, TRG_dataset)\n",
    "len(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venture \n",
      "marry \n",
      "is \n",
      "recurring \n",
      "also \n",
      "get \n",
      "my \n",
      "couple \n",
      "blamed \n",
      "gross \n",
      "surface \n",
      "bush \n",
      "listed \n",
      "concerts \n",
      "appearance \n",
      "gala \n",
      "husband \n",
      "baby \n",
      "dedicated \n",
      "cries \n",
      "raise \n",
      "ballot \n",
      "asked \n",
      "encourage \n",
      "addressed \n",
      "recipients \n",
      "spend \n",
      "recordings \n",
      "proves \n",
      "aggressive \n",
      "museums \n",
      "latest \n",
      "rush \n",
      "pulse \n",
      "services \n",
      "shoe \n",
      "disaster \n",
      "initially \n",
      "miss \n",
      "guest \n",
      "related \n",
      "surviving \n",
      "painted \n",
      "portraits \n",
      "intimate \n",
      "report \n",
      "delicate \n",
      "commented \n",
      "moment \n",
      "crushed \n",
      "officially \n",
      "identified \n",
      "pupil \n",
      "attendance \n",
      "road \n",
      "displaying \n",
      "think \n",
      "affection \n",
      "recover \n",
      "calm \n",
      "soprano \n",
      "invitation \n",
      "inability \n",
      "showed \n",
      "dozen \n",
      "essay \n",
      "diagnosis \n",
      "certificate \n",
      "descriptive \n",
      "scholarly \n",
      "flexible \n",
      "straightforward \n",
      "formidable \n",
      "increases \n",
      "modes \n",
      "disregard \n",
      "lucky \n",
      "comfortable \n",
      "peers \n",
      "publisher \n",
      "seventeen \n",
      "barely \n",
      "themselves \n",
      "mentioned \n",
      "care \n",
      "strongly \n",
      "dominate \n",
      "replace \n",
      "workshops \n",
      "recovery \n",
      "raids \n",
      "organic \n",
      "priest \n",
      "trips \n",
      "embassy \n",
      "nominal \n",
      "belonged \n",
      "amplifier \n",
      "pulled \n",
      "stereo \n",
      "cars \n",
      "computer \n",
      "lapse \n",
      "infringement \n",
      "behalf \n",
      "72 \n",
      "relating \n",
      "somewhat \n",
      "wind \n",
      "analogous \n",
      "interaction \n",
      "mechanic \n",
      "speaker \n",
      "spirits \n",
      "ranch \n",
      "sacrifices \n",
      "suited \n",
      "pointing \n",
      "bundles \n",
      "reward \n",
      "portrays \n",
      "screening \n",
      "holiday \n",
      "assassin \n",
      "litigation \n",
      "verified \n",
      "entering \n",
      "posted \n",
      "nose \n",
      "knight \n",
      "decades \n",
      "bored \n",
      "filtering \n",
      "rebuild \n",
      "deny \n",
      "destructive \n",
      "cracks \n",
      "survivors \n",
      "economics \n",
      "widespread \n",
      "phones \n",
      "feeds \n",
      "stories \n",
      "poorly \n",
      "inspect \n",
      "traded \n",
      "trial \n",
      "structures \n",
      "pressures \n",
      "unprecedented \n",
      "avenue \n",
      "revenues \n",
      "corporation \n",
      "giants \n",
      "tennis \n",
      "anyone \n",
      "drug \n",
      "seems \n",
      "reduction \n",
      "gases \n",
      "discharge \n",
      "classrooms \n",
      "ahead \n",
      "lawyer \n",
      "rescued \n",
      "satire \n",
      "jury \n",
      "factors \n",
      "demonstrate \n",
      "lamb \n",
      "lawyers \n",
      "symbolically \n",
      "notes \n",
      "consultant \n",
      "stored \n",
      "vapor \n",
      "designing \n",
      "cheap \n",
      "placement \n",
      "pond \n",
      "crystalline \n",
      "concrete \n",
      "crop \n",
      "rows \n",
      "enclosed \n",
      "challenge \n",
      "delivering \n",
      "pressing \n",
      "reliance \n",
      "beam \n",
      "imposing \n",
      "reluctantly \n",
      "omitted \n",
      "symphonic \n",
      "recommendation \n",
      "coronary \n",
      "artery \n",
      "operative \n",
      "monastic \n",
      "creator \n",
      "sights \n",
      "corpse \n",
      "encounters \n",
      "dissatisfaction \n",
      "forgiveness \n",
      "attitudes \n",
      "compassion \n",
      "viewing \n",
      "optimistic \n",
      "interdependent \n",
      "attaining \n",
      "purified \n",
      "statements \n",
      "intentional \n",
      "ethical \n",
      "instructed \n",
      "hymn \n",
      "linear \n",
      "description \n",
      "broadened \n",
      "cousin \n",
      "scholastic \n",
      "reserved \n",
      "politically \n",
      "judges \n",
      "kicked \n",
      "shock \n",
      "canceled \n",
      "cried \n",
      "sponsor \n",
      "hopes \n",
      "legs \n",
      "susceptible \n",
      "poisonous \n",
      "prevention \n",
      "selective \n",
      "bite \n",
      "commodity \n",
      "jumping \n",
      "ingredients \n",
      "elderly \n",
      "detected \n",
      "rational \n",
      "politicians \n",
      "rendered \n",
      "solitary \n",
      "construct \n",
      "denying \n",
      "ambassador \n",
      "correspondents \n",
      "repeat \n",
      "sexes \n",
      "experimental \n",
      "politician \n",
      "achievement \n",
      "faculty \n",
      "finishing \n",
      "independently \n",
      "tables \n",
      "colonial \n",
      "uneven \n",
      "adjustment \n",
      "poverty \n",
      "equivalent \n",
      "turning \n",
      "provincial \n",
      "technically \n",
      "elite \n",
      "merger \n",
      "tap \n",
      "defendant \n",
      "endeavor \n",
      "peculiar \n",
      "cleaner \n",
      "develops \n",
      "defense \n",
      "boots \n",
      "prompt \n",
      "reserves \n",
      "coats \n",
      "priests \n",
      "exception \n",
      "pastoral \n",
      "steps \n",
      "tends \n",
      "designs \n",
      "potent \n",
      "sight \n",
      "papers \n",
      "attorneys \n",
      "delight \n",
      "practically \n",
      "reactions \n",
      "stimulate \n",
      "originality \n",
      "estates \n",
      "accompanying \n",
      "dash \n",
      "fostered \n",
      "soap \n",
      "horizon \n",
      "struggled \n",
      "monthly \n",
      "aide \n",
      "jet \n",
      "boats \n",
      "approaches \n",
      "1951 \n",
      "39 \n",
      "hardy \n",
      "1793 \n",
      "29th \n",
      "fireworks \n",
      "customs \n",
      "baptized \n",
      "tolerant \n",
      "confrontation \n",
      "waged \n",
      "nationwide \n",
      "commanders \n",
      "physics \n",
      "outward \n",
      "illustrates \n",
      "however \n",
      "subsidiary \n",
      "expired \n",
      "pull \n",
      "fashionable \n",
      "tenant \n",
      "rockets \n",
      "harvest \n",
      "jungle \n",
      "shelters \n",
      "barren \n",
      "interesting \n",
      "arrange \n",
      "examined \n",
      "flourish \n",
      "ambitious \n",
      "sensations \n",
      "cord \n",
      "operators \n",
      "occasional \n",
      "lounge \n",
      "partition \n",
      "habit \n",
      "faithful \n",
      "pie \n",
      "inventory \n",
      "tenants \n",
      "prospect \n",
      "familiar \n",
      "laboratories \n",
      "1925 \n",
      "periodicals \n",
      "provisions \n",
      "voyage \n",
      "tourist \n",
      "risen \n",
      "token \n",
      "airfield \n",
      "unless \n",
      "radioactive \n",
      "anode \n",
      "impurities \n",
      "viscosity \n",
      "geometry \n",
      "suits \n",
      "outer \n",
      "interfaces \n",
      "repaired \n",
      "collections \n",
      "unity \n",
      "postal \n",
      "clients \n",
      "interpretations \n",
      "masculine \n",
      "festivities \n",
      "libraries \n",
      "participates \n",
      "pedestrian \n",
      "feather \n",
      "acid \n",
      "harmonies \n",
      "ballads \n",
      "predominantly \n",
      "sweat \n",
      "hail \n",
      "uncertain \n",
      "sunset \n",
      "dinner \n",
      "cows \n",
      "blindness \n",
      "bore \n",
      "specify \n",
      "predictable \n",
      "preserves \n",
      "compliance \n",
      "sailing \n",
      "flu \n",
      "mast \n",
      "educators \n",
      "surrendered \n",
      "mistakes \n",
      "blossom \n",
      "pretty \n",
      "match \n",
      "hiring \n",
      "spokesman \n",
      "countryside \n",
      "overly \n",
      "pit \n",
      "floors \n",
      "cleaned \n",
      "plentiful \n",
      "conventions \n",
      "rhythms \n",
      "continuous \n",
      "possibilities \n",
      "listened \n",
      "refinement \n",
      "slaughter \n",
      "recognizable \n",
      "wool \n",
      "plague \n",
      "vaults \n",
      "incidents \n",
      "outlined \n",
      "witness \n",
      "inconsistent \n",
      "permitting \n",
      "rider \n",
      "retreating \n",
      "domination \n",
      "scream \n",
      "yelled \n",
      "assemblies \n",
      "1868 \n",
      "reversed \n",
      "oh \n",
      "clarify \n",
      "proceedings \n",
      "veterans \n",
      "halfway \n",
      "herd \n",
      "edges \n",
      "survival \n",
      "skiing \n",
      "correspondence \n",
      "template \n",
      "complexes \n",
      "pepper \n",
      "segments \n",
      "swallowed \n",
      "microscope \n",
      "symmetric \n",
      "neocortex \n",
      "glued \n",
      "indignation \n",
      "permits \n",
      "reviewed \n",
      "supervision \n",
      "servant \n",
      "compelling \n",
      "clues \n",
      "spontaneously \n",
      "proportional \n",
      "hides \n",
      "spreads \n",
      "willow \n",
      "diminishing \n",
      "beds \n",
      "tear \n",
      "pottery \n",
      "embroidered \n",
      "taxation \n",
      "separately \n",
      "calculating \n",
      "embrace \n",
      "bolt \n",
      "pigment \n",
      "hiding \n",
      "asks \n",
      "sixty \n",
      "embassies \n",
      "northward \n",
      "flock \n",
      "cage \n",
      "marrow \n",
      "antigen \n",
      "adolescent \n",
      "excerpts \n",
      "extract \n",
      "seize \n",
      "representing \n",
      "fingers \n",
      "clarity \n",
      "feasible \n",
      "conversation \n",
      "indefinite \n",
      "buffalo \n",
      "graduates \n",
      "depletion \n",
      "shells \n",
      "rails \n",
      "psychologists \n",
      "volatile \n",
      "vibrant \n",
      "marched \n",
      "accomplishments \n",
      "steering \n",
      "criminals \n",
      "responding \n",
      "stole \n",
      "refrain \n",
      "rabbi \n",
      "comb \n",
      "hopped \n",
      "arrives \n",
      "fatal \n",
      "bitterness \n",
      "poured \n",
      "chilled \n",
      "questionnaire \n",
      "tender \n",
      "transaction \n",
      "anchored \n",
      "negotiated \n",
      "260 \n",
      "marksman \n",
      "psychologist \n",
      "sadness \n",
      "gatherings \n",
      "replaces \n",
      "arresting \n",
      "incapable \n",
      "pilots \n",
      "modernization \n",
      "enlarge \n",
      "outspoken \n",
      "noticed \n",
      "enlisted \n",
      "postponed \n",
      "sorry \n",
      "stiff \n",
      "expanse \n",
      "graphite \n",
      "moves \n",
      "trails \n",
      "systematically \n",
      "mankind \n",
      "inactive \n",
      "pending \n",
      "patriot \n",
      "pistol \n",
      "spurred \n",
      "cart \n",
      "wonders \n",
      "perpetual \n",
      "renewed \n",
      "suspicious \n",
      "securing \n",
      "inclined \n",
      "worried \n",
      "shipments \n",
      "muzzle \n",
      "knock \n",
      "coordinates \n",
      "tiles \n",
      "gunfire \n",
      "defective \n",
      "sunburn \n",
      "currents \n",
      "painful \n",
      "seasoned \n",
      "glaze \n",
      "lemon \n",
      "anticipate \n",
      "irregular \n",
      "gown \n",
      "ration \n",
      "spite \n",
      "closet \n",
      "whereby \n",
      "thereby \n",
      "convictions \n",
      "restoring \n",
      "embodiment \n",
      "narrator \n",
      "bacon \n",
      "duck \n",
      "pretend \n",
      "afraid \n",
      "vaudeville \n",
      "divergent \n",
      "paralysis \n",
      "questionable \n",
      "fence \n",
      "sensible \n",
      "leaning \n",
      "immortal \n",
      "conceal \n",
      "anarchy \n",
      "steal \n",
      "horizons \n",
      "upright \n",
      "tournaments \n",
      "thorough \n",
      "substituted \n",
      "phrases \n",
      "cow \n",
      "staircase \n",
      "inert \n",
      "blows \n",
      "electrode \n",
      "stuff \n",
      "tying \n",
      "missed \n",
      "quarrel \n",
      "motivations \n",
      "boast \n",
      "bony \n",
      "maneuver \n",
      "multiplying \n",
      "deviation \n",
      "assured \n",
      "freshman \n",
      "disciplined \n",
      "suddenly \n",
      "gum \n",
      "bearing \n",
      "conferred \n",
      "conspiracy \n",
      "groupings \n",
      "lonely \n",
      "pleading \n",
      "sprayed \n",
      "drain \n",
      "supplier \n",
      "sufficiently \n",
      "spectacular \n",
      "declares \n",
      "apron \n",
      "friction \n",
      "liquidated \n",
      "congressmen \n",
      "cured \n",
      "28th \n",
      "advocating \n",
      "wonder \n",
      "contradiction \n",
      "brigadier \n",
      "equation \n",
      "rebuilding \n",
      "par \n",
      "cumulative \n",
      "competent \n",
      "approximated \n",
      "grinding \n",
      "cheerful \n",
      "sophomore \n",
      "probable \n",
      "lately \n",
      "projections \n",
      "adversary \n",
      "outlook \n",
      "withheld \n",
      "smiles \n",
      "virtues \n",
      "distributions \n",
      "surprising \n",
      "entrenched \n",
      "leveling \n",
      "enjoys \n",
      "lagoon \n",
      "outbursts \n",
      "skies \n",
      "locks \n",
      "sportsmen \n",
      "flesh \n",
      "neurotic \n",
      "deduced \n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "embeddings_index = {}\n",
    "\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((trainVocab.count + 2, max_length))\n",
    "for i in range(len(trainVocab.words)):\n",
    "    word = trainVocab.index[str(i)]\n",
    "    try:\n",
    "        embedding_vector = w2v.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        print(\"{} \".format(word))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, n_layers, dropout = 0.1):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "                \n",
    "        # self.hidden = torch.zeros(n_layers, 1, hidden_size)\n",
    "        # self.cell = torch.zeros(n_layers, 1, hidden_size)\n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        # EXTRA CREDIT: Load your pretrained embeddings into the LSTM unit -- how?\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(embedding_matrix))\n",
    "        self.embedding_dim = self.embedding.weight.shape[1]\n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_size, n_layers, dropout = dropout)\n",
    "    \n",
    "    def forward(self, i, hidden, cell):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the src vector\n",
    "        Outputs: o, the encoder outputs\n",
    "                h, the hidden state\n",
    "                c, the cell state\n",
    "        '''\n",
    "        i = i.to(torch.int)\n",
    "        lstm_in = self.embedding(i)\n",
    "        # print(i)\n",
    "        o, (h, c) = self.lstm(lstm_in, (hidden, cell))\n",
    "        return o, h, c\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, n_layers, dropout_p):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(embedding_matrix))\n",
    "        self.embedding_size = self.embedding.weight.shape[1]\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, n_layers, dropout = dropout_p)\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer     \n",
    "        self.output = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        # self.hidden = torch.zeros(n_layers, 1, hidden_size)\n",
    "        # self.cell = torch.zeros(n_layers, 1, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, i, h, c):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''\n",
    "        # i dim: [batch size]\n",
    "        # h dim: [n_layers, batch_size, hidden_size]\n",
    "        # print(\"i shape: \", i.shape)\n",
    "        i = i.to(torch.int)\n",
    "\n",
    "        input = i.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded, (h, c))\n",
    "        o = self.output(lstm_out)\n",
    "        return o, hidden, cell\n",
    "        \n",
    "        \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_input_size, encoder_hidden_size, decoder_hidden_size, decoder_output_size):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.n_layers = 2\n",
    "        self.encoder = Encoder(encoder_input_size, encoder_hidden_size, self.n_layers, dropout = 0.1)\n",
    "        self.decoder = Decoder(decoder_hidden_size, decoder_output_size, self.n_layers, dropout_p = 0.1)\n",
    "        if (torch.cuda.is_available()):\n",
    "            print(\"cuda available\")\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            print(\"cuda unavailable\")\n",
    "            self.device = torch.device('cpu')\n",
    "    \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):      \n",
    "        batch_size = trg.shape[1]\n",
    "        # print(\"trg shape: \", trg.shape)\n",
    "        # print(\"batch size: \", batch_size)\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "\n",
    "        # tensor to store decoder outputs\n",
    "        o = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden state of the encoder serves as the initial hidden state of the decoder\n",
    "        encoder_hidden = torch.zeros(self.n_layers, batch_size, self.encoder.hidden_size).to(self.device)\n",
    "        encoder_cell = torch.zeros(self.n_layers, batch_size, self.encoder.hidden_size).to(self.device)\n",
    "\n",
    "        out, hidden, cell = self.encoder(src, encoder_hidden, encoder_cell)\n",
    "        # first input to the decoder is <sos> tokens\n",
    "        # print(\"encoder passed\")\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # print(\"decoder passed {} times among {}\".format(t, trg_len))\n",
    "            o[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(2)\n",
    "            # print(\"output shape: \", output.shape)\n",
    "            # print(\"top1: \", top1)\n",
    "            \n",
    "            if (teacher_force):\n",
    "                input = trg[t]\n",
    "            else:\n",
    "                input = top1\n",
    "                input = input.squeeze(0)\n",
    "\n",
    "        return o\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n",
      "model moved to cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(2865, 100)\n",
       "    (lstm): LSTM(100, 512, num_layers=2, dropout=0.1)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2865, 100)\n",
       "    (lstm): LSTM(100, 512, num_layers=2, dropout=0.1)\n",
       "    (output): Linear(in_features=512, out_features=2864, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(trainVocab.words) + 1\n",
    "output_size = len(trainVocab.words) + 1\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "seq = Seq2Seq(input_size, hidden_dim, hidden_dim, output_size)\n",
    "emb_dim = seq.encoder.embedding_dim\n",
    "seq = seq.to(seq.device)\n",
    "print(\"model moved to {}\".format(seq.device))\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if (name == \"embedding.weight\"):\n",
    "            continue\n",
    "        \n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "seq.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "optimizer = optim.Adam(seq.parameters())\n",
    "# trg_pad_idx = [trainVocab.words['sos'], trainVocab.words['eos']]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(log_dir='./tensorboard/')\n",
    "\n",
    "def train(model, iterator, optimizer, critierion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(tqdm(iterator)):\n",
    "        src = batch['question']\n",
    "        trg = batch['answer']\n",
    "\n",
    "        # print(\"src shape: \", src.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "        \n",
    "            src = batch['question']\n",
    "            trg = batch['answer']\n",
    "            # print(src.shape)\n",
    "            src = src.to(model.device)\n",
    "            trg = src.to(model.device)\n",
    "\n",
    "            output = model(src, trg, 0) # turn off teacher forcing\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        \n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1085 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 69456\n",
      "Validation Data Size : 8682\n",
      "Testing Data Size : 8683\n",
      "Training Data num Batches : 1085\n",
      "Validation Data num Batches : 135\n",
      "Testing Data num Batches : 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:04<00:00,  8.68it/s]\n",
      "  0%|          | 1/1085 [00:00<02:02,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 9s\n",
      "\tTrain Loss: 0.345 \n",
      "\t Val. Loss: 1.282 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:05<00:00,  8.66it/s]\n",
      "  0%|          | 1/1085 [00:00<02:22,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 2m 9s\n",
      "\tTrain Loss: 0.284 \n",
      "\t Val. Loss: 1.366 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:04<00:00,  8.71it/s]\n",
      "  0%|          | 1/1085 [00:00<01:50,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 2m 9s\n",
      "\tTrain Loss: 0.286 \n",
      "\t Val. Loss: 2.034 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:05<00:00,  8.67it/s]\n",
      "  0%|          | 1/1085 [00:00<02:33,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 2m 9s\n",
      "\tTrain Loss: 0.282 \n",
      "\t Val. Loss: 1.256 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:04<00:00,  8.72it/s]\n",
      "  0%|          | 1/1085 [00:00<02:18,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 2m 8s\n",
      "\tTrain Loss: 0.282 \n",
      "\t Val. Loss: 2.012 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:05<00:00,  8.68it/s]\n",
      "  0%|          | 1/1085 [00:00<02:04,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Time: 2m 9s\n",
      "\tTrain Loss: 0.279 \n",
      "\t Val. Loss: 1.880 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:05<00:00,  8.67it/s]\n",
      "  0%|          | 0/1085 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Time: 2m 9s\n",
      "\tTrain Loss: 0.277 \n",
      "\t Val. Loss: 1.304 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:04<00:00,  8.71it/s]\n",
      "  0%|          | 0/1085 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Time: 2m 9s\n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 1.874 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:04<00:00,  8.71it/s]\n",
      "  0%|          | 0/1085 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Time: 2m 9s\n",
      "\tTrain Loss: 0.279 \n",
      "\t Val. Loss: 1.384 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:04<00:00,  8.69it/s]\n",
      "  0%|          | 0/1085 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Time: 2m 9s\n",
      "\tTrain Loss: 0.279 \n",
      "\t Val. Loss: 1.436 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:04<00:00,  8.71it/s]\n",
      "  0%|          | 1/1085 [00:00<02:08,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Time: 2m 8s\n",
      "\tTrain Loss: 0.276 \n",
      "\t Val. Loss: 1.351 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [02:05<00:00,  8.66it/s]\n",
      "  0%|          | 1/1085 [00:00<02:23,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Time: 2m 9s\n",
      "\tTrain Loss: 0.273 \n",
      "\t Val. Loss: 1.401 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 563/1085 [01:04<01:03,  8.19it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5a469207ba94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-527e3126f18b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, critierion, clip)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_dataset, validation_dataset, test_dataset = train_test_split(dataset)\n",
    "print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "print(f\"Validation Data Size : {len(validation_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(test_dataset)}\")\n",
    "\n",
    "def make_batch(samples):\n",
    "    max_batch_data_size = 30\n",
    "    questions = [sample['question'] for sample in samples]\n",
    "    # questions.append(torch.Tensor(np.zeros(30)))\n",
    "    answers = [sample['answer'] for sample in samples]\n",
    "    # answers.append(torch.Tensor(np.zeros(30)))\n",
    "\n",
    "    padded_questions = torch.nn.utils.rnn.pad_sequence(questions, batch_first=False)\n",
    "    # padded_questions = padded_questions[:,0:16]\n",
    "    padded_answers = torch.nn.utils.rnn.pad_sequence(answers, batch_first=False)\n",
    "    # padded_answers = padded_questions[:,0:16]\n",
    "    # print(\"padded questions shape: \", padded_questions.shape)\n",
    "    return {'question': padded_questions.contiguous().to(\"cuda\"),\n",
    "            'answer': padded_answers.contiguous().to(\"cuda\")}\n",
    " # temporary, hard coded to use cuda\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 64, shuffle=True, drop_last=True, collate_fn=make_batch)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size = 64, shuffle=True, drop_last=True, collate_fn=make_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 64, shuffle=True, drop_last=True, collate_fn=make_batch)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training Data num Batches : {len(train_dataloader)}\")\n",
    "print(f\"Validation Data num Batches : {len(validation_dataloader)}\")\n",
    "print(f\"Testing Data num Batches : {len(test_dataloader)}\")\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(seq, train_dataloader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(seq, validation_dataloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq.state_dict(), 'model64_th1e-1_' + str(epoch) + '.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} ')\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} ')\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 1.272\n"
     ]
    }
   ],
   "source": [
    "seq.load_state_dict(torch.load('model64_th1e-1_3.pt'))\n",
    "\n",
    "test_loss = evaluate(seq, test_dataloader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863, 2863]\n",
      "Chatbot:  grammy None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "input = \"what is your name\"\n",
    "input = prepare_text(input)\n",
    "inputInt = []\n",
    "inputInt.append(trainVocab.words['sos'])\n",
    "for t in input:\n",
    "    try:\n",
    "        inputInt.append(trainVocab.words[t.lower()])\n",
    "    except:\n",
    "        inputInt.append(len(trainVocab.words))\n",
    "\n",
    "inputInt.append(trainVocab.words['eos'])\n",
    "npFakeInput = np.zeros((len(inputInt), 64))\n",
    "max_answer_len = 30\n",
    "fake_batch = torch.Tensor(npFakeInput).to(torch.int).to(seq.device)\n",
    "npFakeAns = np.zeros((max_answer_len, 64))\n",
    "fake_ans = torch.Tensor(npFakeAns).to(torch.int).to(seq.device)\n",
    "# fake_ans = torch.cat((fake_batch, torch.Tensor( np.zeros((1, max_answer_len - len(inputInt))) ) ), dim=1)\n",
    "answer = seq(fake_batch, fake_ans, 0)\n",
    "output_dim = answer.shape[-1]\n",
    "answer = answer[:,0,:].view(-1, output_dim)\n",
    "answerIdx = answer.argmax(dim=1)\n",
    "answerIdxList = answerIdx.tolist()\n",
    "print(answerIdxList)\n",
    "ansText = \"\"\n",
    "\n",
    "for idx in answerIdxList:\n",
    "    if idx >= trainVocab.count:\n",
    "        ansText = ansText + \" None\"\n",
    "        continue\n",
    "    ansText = ansText + \" \" + trainVocab.index[str(idx)]\n",
    "print(\"Chatbot: \" + ansText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
